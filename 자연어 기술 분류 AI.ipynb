{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import autokeras as ak\n",
    "import konlpy, re, tqdm, os\n",
    "from keras import losses\n",
    "import keras\n",
    "from konlpy.tag import Mecab\n",
    "import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def Find_Optimal_Cutoff(target, predicted):\n",
    "\n",
    "    fpr, tpr, threshold = roc_curve(target, predicted)\n",
    "    i = np.arange(len(tpr))\n",
    "    roc = pd.DataFrame({'tf' : pd.Series(tpr-(1-fpr), index = i), 'threshold' : pd.Series(threshold, index = i)})\n",
    "    roc_t = roc.iloc[(roc.tf-0).abs().argsort()[:1]]\n",
    "    return list(roc_t['threshold'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '/Users/roopre/Desktop/STUDY/dacon/NLP Baseline code(수정)/open/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(PATH+'train.csv')\n",
    "test=pd.read_csv(PATH+'test.csv')\n",
    "sample_submission=pd.read_csv(PATH+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = \"과제명 : \"+ train['과제명'] + \" 요약문_키워드 : \" + train['요약문_한글키워드'].fillna('.').astype('str')\n",
    "y_train = train['label'].copy()\n",
    "y_train_1 = train['label']\n",
    "x_test  = \"과제명 : \"+ test['과제명'] + \" 요약문_키워드 : \" + test['요약문_한글키워드'].fillna('.').astype('str')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0_1_split = y_train.copy()\n",
    "y_train_0_1_split[train['label']!=0]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_1 = x_train[train['label']!=0]\n",
    "y_train_1 = y_train_1[train['label']!=0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 174304/174304 [01:21<00:00, 2138.36it/s]\n",
      "100%|██████████| 31733/31733 [00:15<00:00, 2112.08it/s]\n",
      "100%|██████████| 43576/43576 [00:21<00:00, 2001.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data : 174304\n",
      "Train data : 31733\n",
      "Test data  : 43576\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def Preprocessing(text, tagger, remove_stopwords=False, stop_words=[]) :\n",
    "    # 한글 정규식 정의(띄어쓰기, ㄱ ~ ㅣ, 가 ~ 힣)\n",
    "    text = re.sub('[^ ㄱ-ㅣ가-힣]+','',text)\n",
    "    \n",
    "    # 텍스트를 형태소로 분리후 각 단어로부터 어간을 추출.(stem=True)\n",
    "    word_text = tagger.morphs(text)\n",
    "    \n",
    "    # 불용어 처리.\n",
    "    if remove_stopwords :\n",
    "        word_text = [ t for t in word_text if not t in stop_words ]\n",
    "        \n",
    "    return word_text\n",
    "\n",
    "# 한글 불용어 사전 파일 이용.\n",
    "with open('./korean_stopwords.txt', encoding='utf-8') as fp :\n",
    "    stop_words = fp.readlines()\n",
    "    \n",
    "# \\n 제거.\n",
    "stop_words = [ x.strip() for x in stop_words ]\n",
    "# print(stop_words)\n",
    "# print('-'*135)\n",
    "# print()\n",
    "\n",
    "# Okt 이용.\n",
    "tagger = Mecab()\n",
    "\n",
    "# train, test 데이터를 정제한 텍스트를 담을 리스트 생성.\n",
    "Clean_train_01_data = []\n",
    "Clean_train_1_data = []\n",
    "Clean_test_data  = []\n",
    "\n",
    "# Train dataset 정제 작업 시작.\n",
    "for text in tqdm.tqdm(x_train) :\n",
    "    try :\n",
    "        Clean_train_01_data.append(Preprocessing(text, tagger, remove_stopwords=True, stop_words=stop_words))\n",
    "    except :\n",
    "        Clean_train_01_data.append([])\n",
    "        \n",
    "for text in tqdm.tqdm(x_train_1) :\n",
    "    try :\n",
    "        Clean_train_1_data.append(Preprocessing(text, tagger, remove_stopwords=True, stop_words=stop_words))\n",
    "    except :\n",
    "        Clean_train_1_data.append([])\n",
    "\n",
    "# est dataset 정제 작업 시작.\n",
    "for text in tqdm.tqdm(x_test) :\n",
    "    if type(text) == str :\n",
    "        Clean_test_data.append(Preprocessing(text, tagger, remove_stopwords=True, stop_words=stop_words))\n",
    "    else :\n",
    "        Clean_test_data.append([])   \n",
    "        \n",
    "# 정제된 데이터의 수.\n",
    "print(f'Train data : {len(Clean_train_01_data)}')\n",
    "print(f'Train data : {len(Clean_train_1_data)}')\n",
    "print(f'Test data  : {len(Clean_test_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 단어의 개수 : 102\n",
      "45391\n"
     ]
    }
   ],
   "source": [
    "#------------ 0, 1구분 훈련\n",
    "# 우선 특별한 옵션을 지정하지 않고 진행.\n",
    "token = Tokenizer()\n",
    "\n",
    "# fit_on_texts() : 입력으로 들어온 텍스트에서 단어의 빈도수가 높은 순으로 낮은 숫자부터 인덱스 부여, 단어 집합 생성.\n",
    "token.fit_on_texts(Clean_train_01_data)\n",
    "\n",
    "# 단어 사전을 통해 문장의 각 단어를 숫자(시퀀스 형태)로 변환. : 인덱스로만 채워진 새로운 배열을 생성한다는 의미.\n",
    "Train_squences = token.texts_to_sequences(Clean_train_01_data)\n",
    "Test_squences  = token.texts_to_sequences(Clean_test_data)\n",
    "\n",
    "# 한 문장의 최대 단어 수를 가져옴.\n",
    "max_cnt = 0\n",
    "for c in Train_squences :\n",
    "    # 현재 문장의 글자수.\n",
    "    cnt = len(c)\n",
    "    # 현재 문장의 글자수가 이전 최대 수치보다 많으면 덮어쓰기.\n",
    "    if max_cnt < cnt :\n",
    "        max_cnt = cnt\n",
    "print(f'최대 단어의 개수 : {max_cnt}')\n",
    "\n",
    "# Padding 처리 : 서로 길이가 다른 리스트의 개수를 max_cnt로 맞춰줌.\n",
    "# 끝 부분으로 적용. ==> 즉 max_cnt 길이의 리스트로 동일하게 맞춰주기 위해 부족한 부분으로 뒤에서 부터 0으로 채우는 작업을 의미.\n",
    "padded_01_train = pad_sequences(Train_squences, max_cnt, padding='post')\n",
    "padded_test  = pad_sequences(Test_squences, max_cnt, padding='post')\n",
    "\n",
    "# One-hot-encoding.\n",
    "# 주의 : 맨 앞에 0이 추가됨.\n",
    "voca_size = len(token.word_index) + 1\n",
    "print(voca_size)\n",
    "# train_x = to_categorical(Train_squences, num_classes = voca_size)\n",
    "# test_x  = to_categorical(Test_squences, num_classes = voca_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 102, 64)           2905024   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_1 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 3,102,657\n",
      "Trainable params: 3,102,657\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "voca_size = 45391\n",
    "max_cnt=102\n",
    "model_011 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(voca_size, embedding_dim, input_length=max_cnt),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model_011.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "print(model_011.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "2179/2179 - 59s - loss: 0.2857 - accuracy: 0.8813 - val_loss: 0.2587 - val_accuracy: 0.8929\n",
      "Epoch 2/50\n",
      "2179/2179 - 57s - loss: 0.2174 - accuracy: 0.9099 - val_loss: 0.2507 - val_accuracy: 0.8976\n",
      "Epoch 3/50\n",
      "2179/2179 - 57s - loss: 0.1843 - accuracy: 0.9238 - val_loss: 0.2415 - val_accuracy: 0.9048\n",
      "Epoch 4/50\n",
      "2179/2179 - 57s - loss: 0.1622 - accuracy: 0.9330 - val_loss: 0.2469 - val_accuracy: 0.9058\n",
      "Epoch 5/50\n",
      "2179/2179 - 57s - loss: 0.1455 - accuracy: 0.9398 - val_loss: 0.2781 - val_accuracy: 0.9086\n",
      "Epoch 6/50\n",
      "2179/2179 - 53s - loss: 0.1304 - accuracy: 0.9462 - val_loss: 0.3076 - val_accuracy: 0.9110\n",
      "Epoch 7/50\n",
      "2179/2179 - 58s - loss: 0.1173 - accuracy: 0.9512 - val_loss: 0.3234 - val_accuracy: 0.9098\n",
      "Epoch 8/50\n",
      "2179/2179 - 57s - loss: 0.1065 - accuracy: 0.9554 - val_loss: 0.3473 - val_accuracy: 0.9113\n",
      "Epoch 9/50\n",
      "2179/2179 - 57s - loss: 0.0939 - accuracy: 0.9608 - val_loss: 0.3438 - val_accuracy: 0.9115\n",
      "Epoch 10/50\n",
      "2179/2179 - 57s - loss: 0.0848 - accuracy: 0.9650 - val_loss: 0.3434 - val_accuracy: 0.9131\n",
      "Epoch 11/50\n",
      "2179/2179 - 57s - loss: 0.0765 - accuracy: 0.9686 - val_loss: 0.3480 - val_accuracy: 0.9155\n",
      "Epoch 12/50\n",
      "2179/2179 - 57s - loss: 0.0686 - accuracy: 0.9720 - val_loss: 0.3900 - val_accuracy: 0.9112\n",
      "Epoch 13/50\n",
      "2179/2179 - 57s - loss: 0.0619 - accuracy: 0.9748 - val_loss: 0.4537 - val_accuracy: 0.9141\n",
      "Epoch 14/50\n",
      "2179/2179 - 57s - loss: 0.0572 - accuracy: 0.9765 - val_loss: 0.5184 - val_accuracy: 0.9145\n",
      "Epoch 15/50\n",
      "2179/2179 - 57s - loss: 0.0518 - accuracy: 0.9788 - val_loss: 0.4650 - val_accuracy: 0.9157\n",
      "Epoch 16/50\n",
      "2179/2179 - 57s - loss: 0.0472 - accuracy: 0.9803 - val_loss: 0.5722 - val_accuracy: 0.9183\n",
      "Epoch 17/50\n",
      "2179/2179 - 57s - loss: 0.0449 - accuracy: 0.9814 - val_loss: 0.5386 - val_accuracy: 0.9150\n",
      "Epoch 18/50\n",
      "2179/2179 - 57s - loss: 0.0409 - accuracy: 0.9828 - val_loss: 0.4737 - val_accuracy: 0.9176\n",
      "Epoch 19/50\n",
      "2179/2179 - 57s - loss: 0.0382 - accuracy: 0.9840 - val_loss: 0.5356 - val_accuracy: 0.9139\n",
      "Epoch 20/50\n",
      "2179/2179 - 57s - loss: 0.0349 - accuracy: 0.9851 - val_loss: 0.6338 - val_accuracy: 0.9178\n",
      "Epoch 21/50\n",
      "2179/2179 - 57s - loss: 0.0331 - accuracy: 0.9857 - val_loss: 0.7417 - val_accuracy: 0.9175\n",
      "Epoch 22/50\n",
      "2179/2179 - 57s - loss: 0.0321 - accuracy: 0.9862 - val_loss: 0.6810 - val_accuracy: 0.9122\n",
      "Epoch 23/50\n",
      "2179/2179 - 57s - loss: 0.0300 - accuracy: 0.9870 - val_loss: 0.7675 - val_accuracy: 0.9137\n",
      "Epoch 24/50\n",
      "2179/2179 - 57s - loss: 0.0281 - accuracy: 0.9877 - val_loss: 0.7887 - val_accuracy: 0.9192\n",
      "Epoch 25/50\n",
      "2179/2179 - 57s - loss: 0.0279 - accuracy: 0.9876 - val_loss: 0.8151 - val_accuracy: 0.9170\n",
      "Epoch 26/50\n",
      "2179/2179 - 57s - loss: 0.0267 - accuracy: 0.9880 - val_loss: 0.7091 - val_accuracy: 0.9174\n",
      "Epoch 27/50\n",
      "2179/2179 - 57s - loss: 0.0254 - accuracy: 0.9886 - val_loss: 0.7779 - val_accuracy: 0.9158\n",
      "Epoch 28/50\n",
      "2179/2179 - 57s - loss: 0.0247 - accuracy: 0.9886 - val_loss: 0.7226 - val_accuracy: 0.9145\n",
      "Epoch 29/50\n",
      "2179/2179 - 57s - loss: 0.0240 - accuracy: 0.9891 - val_loss: 0.6999 - val_accuracy: 0.9092\n",
      "Epoch 30/50\n",
      "2179/2179 - 57s - loss: 0.0229 - accuracy: 0.9893 - val_loss: 0.8990 - val_accuracy: 0.9145\n",
      "Epoch 31/50\n",
      "2179/2179 - 57s - loss: 0.0216 - accuracy: 0.9899 - val_loss: 0.9377 - val_accuracy: 0.9178\n",
      "Epoch 32/50\n",
      "2179/2179 - 57s - loss: 0.0213 - accuracy: 0.9896 - val_loss: 0.9103 - val_accuracy: 0.9156\n",
      "Epoch 33/50\n",
      "2179/2179 - 57s - loss: 0.0207 - accuracy: 0.9902 - val_loss: 0.8467 - val_accuracy: 0.9153\n",
      "Epoch 34/50\n",
      "2179/2179 - 57s - loss: 0.0205 - accuracy: 0.9904 - val_loss: 0.9349 - val_accuracy: 0.9180\n",
      "Epoch 35/50\n",
      "2179/2179 - 57s - loss: 0.0199 - accuracy: 0.9905 - val_loss: 0.8670 - val_accuracy: 0.9129\n",
      "Epoch 36/50\n",
      "2179/2179 - 57s - loss: 0.0193 - accuracy: 0.9905 - val_loss: 0.9664 - val_accuracy: 0.9142\n",
      "Epoch 37/50\n",
      "2179/2179 - 57s - loss: 0.0185 - accuracy: 0.9912 - val_loss: 0.9764 - val_accuracy: 0.9148\n",
      "Epoch 38/50\n",
      "2179/2179 - 57s - loss: 0.0190 - accuracy: 0.9908 - val_loss: 0.8069 - val_accuracy: 0.9150\n",
      "Epoch 39/50\n",
      "2179/2179 - 57s - loss: 0.0177 - accuracy: 0.9912 - val_loss: 0.9453 - val_accuracy: 0.9168\n",
      "Epoch 40/50\n",
      "2179/2179 - 57s - loss: 0.0177 - accuracy: 0.9914 - val_loss: 1.0327 - val_accuracy: 0.9181\n",
      "Epoch 41/50\n",
      "2179/2179 - 57s - loss: 0.0174 - accuracy: 0.9914 - val_loss: 0.8609 - val_accuracy: 0.9173\n",
      "Epoch 42/50\n",
      "2179/2179 - 57s - loss: 0.0169 - accuracy: 0.9917 - val_loss: 1.1457 - val_accuracy: 0.9197\n",
      "Epoch 43/50\n",
      "2179/2179 - 57s - loss: 0.0168 - accuracy: 0.9917 - val_loss: 0.9927 - val_accuracy: 0.9171\n",
      "Epoch 44/50\n",
      "2179/2179 - 55s - loss: 0.0171 - accuracy: 0.9916 - val_loss: 0.8950 - val_accuracy: 0.9190\n",
      "Epoch 45/50\n",
      "2179/2179 - 57s - loss: 0.0166 - accuracy: 0.9917 - val_loss: 1.0631 - val_accuracy: 0.9141\n",
      "Epoch 46/50\n",
      "2179/2179 - 57s - loss: 0.0163 - accuracy: 0.9919 - val_loss: 0.9300 - val_accuracy: 0.9155\n",
      "Epoch 47/50\n",
      "2179/2179 - 57s - loss: 0.0161 - accuracy: 0.9920 - val_loss: 0.9235 - val_accuracy: 0.9172\n",
      "Epoch 48/50\n",
      "2179/2179 - 57s - loss: 0.0163 - accuracy: 0.9917 - val_loss: 1.0671 - val_accuracy: 0.9141\n",
      "Epoch 49/50\n",
      "2179/2179 - 57s - loss: 0.0156 - accuracy: 0.9921 - val_loss: 1.2175 - val_accuracy: 0.9195\n",
      "Epoch 50/50\n",
      "2179/2179 - 52s - loss: 0.0159 - accuracy: 0.9919 - val_loss: 1.1238 - val_accuracy: 0.9191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe61e8bce50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "model_011.fit(padded_01_train, y_train_0_1_split, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    batch_size=64,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 단어의 개수 : 73\n",
      "19272\n"
     ]
    }
   ],
   "source": [
    "#------------ 0, 1구분 훈련\n",
    "# 우선 특별한 옵션을 지정하지 않고 진행.\n",
    "token = Tokenizer()\n",
    "\n",
    "# fit_on_texts() : 입력으로 들어온 텍스트에서 단어의 빈도수가 높은 순으로 낮은 숫자부터 인덱스 부여, 단어 집합 생성.\n",
    "token.fit_on_texts(Clean_train_1_data)\n",
    "\n",
    "# 단어 사전을 통해 문장의 각 단어를 숫자(시퀀스 형태)로 변환. : 인덱스로만 채워진 새로운 배열을 생성한다는 의미.\n",
    "Train_squences = token.texts_to_sequences(Clean_train_1_data)\n",
    "Test_squences_1  = token.texts_to_sequences(Clean_test_data)\n",
    "\n",
    "# 한 문장의 최대 단어 수를 가져옴.\n",
    "max_cnt = 0\n",
    "for c in Train_squences :\n",
    "    # 현재 문장의 글자수.\n",
    "    cnt = len(c)\n",
    "    # 현재 문장의 글자수가 이전 최대 수치보다 많으면 덮어쓰기.\n",
    "    if max_cnt < cnt :\n",
    "        max_cnt = cnt\n",
    "print(f'최대 단어의 개수 : {max_cnt}')\n",
    "\n",
    "# Padding 처리 : 서로 길이가 다른 리스트의 개수를 max_cnt로 맞춰줌.\n",
    "# 끝 부분으로 적용. ==> 즉 max_cnt 길이의 리스트로 동일하게 맞춰주기 위해 부족한 부분으로 뒤에서 부터 0으로 채우는 작업을 의미.\n",
    "padded_1_train = pad_sequences(Train_squences, max_cnt, padding='post')\n",
    "padded_1_test = pad_sequences(Test_squences_1, max_cnt, padding='post')\n",
    "#padded_test  = pad_sequences(Test_squences, max_cnt, padding='post')\n",
    "\n",
    "# One-hot-encoding.\n",
    "# 주의 : 맨 앞에 0이 추가됨.\n",
    "voca_size = len(token.word_index) + 1\n",
    "print(voca_size)\n",
    "# train_x = to_categorical(Train_squences, num_classes = voca_size)\n",
    "# test_x  = to_categorical(Test_squences, num_classes = voca_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 73, 64)            1233408   \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d_2 ( (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 512)               33280     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 46)                5934      \n",
      "=================================================================\n",
      "Total params: 1,436,846\n",
      "Trainable params: 1,436,846\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "embedding_dim = 64\n",
    "voca_size = 19272\n",
    "max_cnt=73\n",
    "model_1 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(voca_size, embedding_dim, input_length=max_cnt),\n",
    "    tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(46, activation='softmax')\n",
    "])\n",
    "\n",
    "# compile model\n",
    "model_1.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# model summary\n",
    "print(model_1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "794/794 - 8s - loss: 2.6387 - accuracy: 0.2598 - val_loss: 1.7538 - val_accuracy: 0.4931\n",
      "Epoch 2/50\n",
      "794/794 - 7s - loss: 1.3406 - accuracy: 0.6129 - val_loss: 1.3478 - val_accuracy: 0.6217\n",
      "Epoch 3/50\n",
      "794/794 - 6s - loss: 0.9293 - accuracy: 0.7394 - val_loss: 1.1889 - val_accuracy: 0.6923\n",
      "Epoch 4/50\n",
      "794/794 - 7s - loss: 0.6763 - accuracy: 0.8114 - val_loss: 1.1103 - val_accuracy: 0.7300\n",
      "Epoch 5/50\n",
      "794/794 - 8s - loss: 0.5279 - accuracy: 0.8550 - val_loss: 1.1311 - val_accuracy: 0.7378\n",
      "Epoch 6/50\n",
      "794/794 - 8s - loss: 0.4356 - accuracy: 0.8797 - val_loss: 1.2740 - val_accuracy: 0.7150\n",
      "Epoch 7/50\n",
      "794/794 - 8s - loss: 0.3690 - accuracy: 0.8973 - val_loss: 1.2305 - val_accuracy: 0.7435\n",
      "Epoch 8/50\n",
      "794/794 - 8s - loss: 0.3251 - accuracy: 0.9057 - val_loss: 1.2601 - val_accuracy: 0.7563\n",
      "Epoch 9/50\n",
      "794/794 - 8s - loss: 0.2760 - accuracy: 0.9215 - val_loss: 1.2754 - val_accuracy: 0.7656\n",
      "Epoch 10/50\n",
      "794/794 - 8s - loss: 0.2431 - accuracy: 0.9295 - val_loss: 1.3248 - val_accuracy: 0.7726\n",
      "Epoch 11/50\n",
      "794/794 - 8s - loss: 0.2217 - accuracy: 0.9349 - val_loss: 1.4236 - val_accuracy: 0.7722\n",
      "Epoch 12/50\n",
      "794/794 - 8s - loss: 0.1962 - accuracy: 0.9423 - val_loss: 1.4507 - val_accuracy: 0.7692\n",
      "Epoch 13/50\n",
      "794/794 - 8s - loss: 0.1825 - accuracy: 0.9454 - val_loss: 1.5713 - val_accuracy: 0.7660\n",
      "Epoch 14/50\n",
      "794/794 - 8s - loss: 0.1572 - accuracy: 0.9517 - val_loss: 1.5957 - val_accuracy: 0.7690\n",
      "Epoch 15/50\n",
      "794/794 - 8s - loss: 0.1558 - accuracy: 0.9534 - val_loss: 1.6313 - val_accuracy: 0.7674\n",
      "Epoch 16/50\n",
      "794/794 - 8s - loss: 0.1374 - accuracy: 0.9573 - val_loss: 1.6185 - val_accuracy: 0.7853\n",
      "Epoch 17/50\n",
      "794/794 - 8s - loss: 0.1242 - accuracy: 0.9610 - val_loss: 1.8446 - val_accuracy: 0.7769\n",
      "Epoch 18/50\n",
      "794/794 - 8s - loss: 0.1152 - accuracy: 0.9642 - val_loss: 1.7232 - val_accuracy: 0.7848\n",
      "Epoch 19/50\n",
      "794/794 - 8s - loss: 0.1087 - accuracy: 0.9655 - val_loss: 1.8874 - val_accuracy: 0.7884\n",
      "Epoch 20/50\n",
      "794/794 - 8s - loss: 0.1046 - accuracy: 0.9676 - val_loss: 1.8257 - val_accuracy: 0.7854\n",
      "Epoch 21/50\n",
      "794/794 - 8s - loss: 0.0965 - accuracy: 0.9682 - val_loss: 1.9810 - val_accuracy: 0.7801\n",
      "Epoch 22/50\n",
      "794/794 - 8s - loss: 0.0863 - accuracy: 0.9707 - val_loss: 1.9710 - val_accuracy: 0.7939\n",
      "Epoch 23/50\n",
      "794/794 - 8s - loss: 0.0818 - accuracy: 0.9720 - val_loss: 2.1455 - val_accuracy: 0.7920\n",
      "Epoch 24/50\n",
      "794/794 - 8s - loss: 0.0845 - accuracy: 0.9717 - val_loss: 2.3144 - val_accuracy: 0.7908\n",
      "Epoch 25/50\n",
      "794/794 - 8s - loss: 0.0754 - accuracy: 0.9752 - val_loss: 2.2540 - val_accuracy: 0.7912\n",
      "Epoch 26/50\n",
      "794/794 - 8s - loss: 0.0704 - accuracy: 0.9749 - val_loss: 2.2152 - val_accuracy: 0.7919\n",
      "Epoch 27/50\n",
      "794/794 - 8s - loss: 0.0726 - accuracy: 0.9744 - val_loss: 2.5115 - val_accuracy: 0.7848\n",
      "Epoch 28/50\n",
      "794/794 - 8s - loss: 0.0619 - accuracy: 0.9792 - val_loss: 2.4047 - val_accuracy: 0.7903\n",
      "Epoch 29/50\n",
      "794/794 - 8s - loss: 0.0639 - accuracy: 0.9773 - val_loss: 2.5341 - val_accuracy: 0.7893\n",
      "Epoch 30/50\n",
      "794/794 - 8s - loss: 0.0578 - accuracy: 0.9788 - val_loss: 2.6395 - val_accuracy: 0.7909\n",
      "Epoch 31/50\n",
      "794/794 - 8s - loss: 0.0618 - accuracy: 0.9776 - val_loss: 2.5943 - val_accuracy: 0.7914\n",
      "Epoch 32/50\n",
      "794/794 - 8s - loss: 0.0648 - accuracy: 0.9763 - val_loss: 2.4912 - val_accuracy: 0.7912\n",
      "Epoch 33/50\n",
      "794/794 - 8s - loss: 0.0537 - accuracy: 0.9808 - val_loss: 2.5171 - val_accuracy: 0.7953\n",
      "Epoch 34/50\n",
      "794/794 - 8s - loss: 0.0546 - accuracy: 0.9798 - val_loss: 2.5926 - val_accuracy: 0.7805\n",
      "Epoch 35/50\n",
      "794/794 - 8s - loss: 0.0485 - accuracy: 0.9812 - val_loss: 2.5923 - val_accuracy: 0.7936\n",
      "Epoch 36/50\n",
      "794/794 - 8s - loss: 0.0535 - accuracy: 0.9804 - val_loss: 2.6938 - val_accuracy: 0.7835\n",
      "Epoch 37/50\n",
      "794/794 - 8s - loss: 0.0550 - accuracy: 0.9796 - val_loss: 2.7355 - val_accuracy: 0.7974\n",
      "Epoch 38/50\n",
      "794/794 - 8s - loss: 0.0462 - accuracy: 0.9831 - val_loss: 2.6702 - val_accuracy: 0.7996\n",
      "Epoch 39/50\n",
      "794/794 - 8s - loss: 0.0458 - accuracy: 0.9822 - val_loss: 3.0281 - val_accuracy: 0.7979\n",
      "Epoch 40/50\n",
      "794/794 - 8s - loss: 0.0490 - accuracy: 0.9814 - val_loss: 2.9046 - val_accuracy: 0.7919\n",
      "Epoch 41/50\n",
      "794/794 - 8s - loss: 0.0437 - accuracy: 0.9824 - val_loss: 2.4727 - val_accuracy: 0.7857\n",
      "Epoch 42/50\n",
      "794/794 - 8s - loss: 0.0478 - accuracy: 0.9814 - val_loss: 2.8917 - val_accuracy: 0.7933\n",
      "Epoch 43/50\n",
      "794/794 - 8s - loss: 0.0374 - accuracy: 0.9838 - val_loss: 3.0397 - val_accuracy: 0.8031\n",
      "Epoch 44/50\n",
      "794/794 - 8s - loss: 0.0489 - accuracy: 0.9823 - val_loss: 3.1519 - val_accuracy: 0.8023\n",
      "Epoch 45/50\n",
      "794/794 - 8s - loss: 0.0428 - accuracy: 0.9831 - val_loss: 3.0787 - val_accuracy: 0.7966\n",
      "Epoch 46/50\n",
      "794/794 - 8s - loss: 0.0353 - accuracy: 0.9851 - val_loss: 3.0525 - val_accuracy: 0.8013\n",
      "Epoch 47/50\n",
      "794/794 - 8s - loss: 0.0452 - accuracy: 0.9818 - val_loss: 2.9446 - val_accuracy: 0.7941\n",
      "Epoch 48/50\n",
      "794/794 - 8s - loss: 0.0366 - accuracy: 0.9841 - val_loss: 3.1164 - val_accuracy: 0.7975\n",
      "Epoch 49/50\n",
      "794/794 - 8s - loss: 0.0430 - accuracy: 0.9829 - val_loss: 3.1415 - val_accuracy: 0.8010\n",
      "Epoch 50/50\n",
      "794/794 - 8s - loss: 0.0362 - accuracy: 0.9845 - val_loss: 2.9480 - val_accuracy: 0.7960\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fe61e8a4160>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "model_1.fit(padded_1_train, y_train_1, \n",
    "                    epochs=num_epochs, verbose=2, \n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 1 구분 threshold 구하기\n",
    "train_pred = model_011.predict(padded_01_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = np.array(train_pred)\n",
    "train_target = np.array(y_train_0_1_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00321844220161438]\n"
     ]
    }
   ],
   "source": [
    "th = Find_Optimal_Cutoff(train_target, train_pred)\n",
    "print(th)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33898\n"
     ]
    }
   ],
   "source": [
    "test_pred_01 = model_011.predict(padded_test)\n",
    "k=[]\n",
    "a=0\n",
    "for i in test_pred_01:\n",
    "    if i < th:\n",
    "        k.append(0)\n",
    "        a+=1\n",
    "    else:\n",
    "        k.append(1)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = pd.DataFrame()\n",
    "tmp['label']=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_1 = padded_1_test[tmp['label']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = model_1.predict(test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_1 = tf.argmax(pred_1,axis=1)\n",
    "rek=np.array(pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 14 23 ... 31  2 19]\n"
     ]
    }
   ],
   "source": [
    "print(rek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "k=np.array(k)\n",
    "for i in range(k.shape[0]):\n",
    "    if k[i] == 1:\n",
    "        k[i] = rek[idx]\n",
    "        idx+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43576,)\n"
     ]
    }
   ],
   "source": [
    "print(k.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.DataFrame()\n",
    "tt['label']=k\n",
    "tt.to_csv('rere.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tmp[tmp['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[tmp['label']==1] = tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp.to_csv('tmppp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt.to_csv('tmp.csv')\n",
    "tmp.to_csv(\"tttt.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['label']=k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv(\"th_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_0_1_split\n",
    "\n",
    "tmpp = np.array(y_train_0_1_split)\n",
    "predd=np.array(pred_01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
